debug: False

base:
  gpu_id: ["0"]
  num_workers : 4
  loss_class: nn.CrossEntropyLoss
  opt_class: AdamW

data:
  text_col: text
  label_col: label
  seed: 777
  use_pseudo_label: False
  n_fold : 5
  train_path : ${store.workdir}/input/train.csv
  test_path : ${store.workdir}/input/test.csv
  max_len : 77
  tokenizer: studio-ousia/luke-japanese-large
  is_train: True

model:
  rnn : None # [None, 'GRU', 'LSTM']
  pooling : mean  # [None, mean, max, attention, max_old, mean_old] # RANDOM SAMPLING
  reinit_layers : -1
  multi_sample_dropout : 0.2
  n_msd : 7  # 5~8
  # ::::::::::::::::::::::::::::::::::::::::::::::::::::
  # optimizer
  # ::::::::::::::::::::::::::::::::::::::::::::::::::::
  encoder_lr : 1e-5
  decoder_lr : 1e-5
  min_lr : 1e-6
  eps : 1e-6
  betas : [0.9, 0.999]
  weight_decay : 0.2
  # ::::::::::::::::::::::::::::::::::::::::::::::::::::
  # scheduler
  # ::::::::::::::::::::::::::::::::::::::::::::::::::::
  scheduler : cosine
  batch_scheduler : True
  num_cycles : 0.5
  num_warmup_steps : 0
  num_train_steps: 
  embedding_size: 512
  is_linear_head: False
  pretrained: studio-ousia/luke-japanese-large
  dropout_rate: 0.0
  num_classes: 2

store:
  workdir: /home/workspace/labo/hatespeech_detection
  model_name: baseline
  root_path: ${store.workdir}/output
  save_path: ${store.root_path}/${store.model_name}/${model.rnn}/${model.pooling}
  model_path: ${store.save_path}/model
  log_path: ${store.save_path}/logs
  result_path: ${store.save_path}/result
  wandb_project: hatespeech

train:
  # trn_fold : [0]
  trn_fold : [0, 1, 2, 3, 4]
  seed: ${data.seed}
  epoch: 4
  batch_size: 16
  max_grad_norm : 1
  gradient_accumulation_steps : 1
  learning_rate: 0.000001
  warm_start: False
  scheduler:
    patience: 5
  callbacks:
    monitor_metric: val_f1
    mode: max
    patience: 2

test:
  batch_size: 32

hydra:
  run:
    dir: ${store.save_path}