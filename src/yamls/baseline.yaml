defaults:
  - _self_

debug: False

base:
  gpu_id: ["0"]
  num_workers: 4
  use_transformer_parameter: True
  do_train: True
  do_test: True

data:
  agg_type: ds #[mv, ds, gl]
  type: label #[label, target]
  label_type: soft #[hard, soft]
  soft_label_col: ${data.agg_type}_soft_${data.type}
  hard_label_col: ${data.agg_type}_hard_${data.type}
  text_col: text
  seed: 777
  train_path: ${store.workdir}/input/train.pkl
  test_path: ${store.workdir}/input/test.pkl
  max_len: 475
  tokenizer: ${model.pretrained}

model:
  rnn: None # [None, 'GRU', 'LSTM']
  pooling: None # [None, mean, max, attention]
  num_reinit_layers: 1 # 0~6
  dropout: 0.2
  n_msd: 6 # 5~8
  mixout: 0 # 0.0~1.0
  pretrained: studio-ousia/luke-japanese-large
  num_classes: 4

scheduler: 
  class_name: cosine
  batch_scheduler: True
  num_cycles: 0.5
  num_warmup_steps: 0
  num_train_steps:

optimizer:
  class_name: AdamW
  encoder_lr: 1e-5
  decoder_lr: 1e-5
  eps: 1e-6
  betas: [0.9, 0.999]
  weight_decay: 0.2

loss:
  class_name: FocalLoss
  params:
    gamma: 10

store:
  workdir: ${oc.env:PWD}
  model_name: ${model.pretrained}
  root_path: ${store.workdir}/output
  save_path: ${store.root_path}/${store.model_name}/${model.mixout}/${data.label_type}/${data.agg_type}/${model.num_reinit_layers}/${model.mixout}${optimizer.decoder_lr}
  model_path: ${store.save_path}/model
  log_path: ${store.save_path}/logs
  result_path: ${store.save_path}/result
  wandb_project: test3

train:
  seed: ${data.seed}
  epoch: 4
  n_fold: 5
  batch_size: 16
  max_grad_norm: 1
  gradient_accumulation_steps: 1
  warm_start: False
  callbacks:
    monitor_metric: epoch
    mode: max
    patience: 2
    save_top_k: 1

test:
  batch_size: 256

hydra:
  run:
    dir: ${store.save_path}
