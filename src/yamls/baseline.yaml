defaults:
  - _self_
  - dataset: hatespeech

debug: False

base:
  gpu_id: ["0"]
  num_workers: 4
  opt_class: AdamW
  loss_class: nn.CrossEntropyLoss
  use_transformer_parameter: True

data:
  text_col: ${dataset.text_col}
  label_col: ${dataset.label_col}
  seed: 777
  use_pseudo_label: False
  n_fold: 5
  train_path: ${store.workdir}/input/${dataset.train_file}
  test_path: ${store.workdir}/input/${dataset.test_file}
  max_len: ${dataset.max_len}
  tokenizer: studio-ousia/luke-japanese-large
  batch_size: 16
  is_train: True

model:
  rnn: GRU # [None, 'GRU', 'LSTM']
  pooling: None # [None, , max, attention]
  reinit_layers: -1
  multi_sample_dropout: 0.2
  n_msd: 7 # 5~8
  embedding_size: 512
  is_linear_head: False
  pretrained: studio-ousia/luke-japanese-large
  dropout_rate: 0.0
  num_classes: 2
  # ::::::::::::::::::::::::::::::::::::::::::::::::::::
  # optimizer
  # ::::::::::::::::::::::::::::::::::::::::::::::::::::
  encoder_lr: 1e-5
  decoder_lr: 1e-5
  min_lr: 1e-6
  eps: 1e-6
  betas: [0.9, 0.999]
  weight_decay: 0.2
  # ::::::::::::::::::::::::::::::::::::::::::::::::::::
  # scheduler
  # ::::::::::::::::::::::::::::::::::::::::::::::::::::
  scheduler: cosine
  batch_scheduler: True
  num_cycles: 0.5
  num_warmup_steps: 0
  num_train_steps:

store:
  workdir: ${oc.env:PWD}
  model_name: baseline
  root_path: ${store.workdir}/output
  save_path: ${store.root_path}/${store.model_name}/${model.rnn}/${model.pooling}
  model_path: ${store.save_path}/model
  log_path: ${store.save_path}/logs
  result_path: ${store.save_path}/result
  wandb_project: 

train:
  trn_fold: [0, 1, 2, 3, 4]
  seed: ${data.seed}
  epoch: 4
  batch_size: 16
  max_grad_norm: 1
  gradient_accumulation_steps: 1
  learning_rate: 0.000001
  warm_start: False
  scheduler:
    patience: 5
  callbacks:
    monitor_metric: epoch
    mode: max
    patience: 2

test:
  batch_size: 256

hydra:
  run:
    dir: ${store.save_path}
