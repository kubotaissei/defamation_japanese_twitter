{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sample Scripts Using Huggingface Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kubotaissei/defamation_japanese_twitter/blob/master/notebooks/train_example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.26 datasets==2.9.0 sentencepiece crowd-kit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug=False\n",
    "    # base\n",
    "    gpu_id=[\"0\"]\n",
    "    output_dir=\"output\"\n",
    "    twitter_bearer_token=\"\"  # Fill in your Twitter API bearer token\n",
    "    # Dataset\n",
    "    agg_type=\"ds\" #[mv, ds, gl] # Label Aggregation Method, Majoryty Voting or Dawid-Skene or GLAD\n",
    "    type=\"label\" #[label, target] # 種類予測 or 対象予測 \n",
    "    label_type=\"soft\" #[hard, soft] # ソフトラベル or ハードラベル\n",
    "    max_len=497\n",
    "    # Model\n",
    "    model=\"studio-ousia/luke-japanese-large\"\n",
    "    num_classes=4\n",
    "    # Scheduler\n",
    "    scheduler_type=\"cosine\"\n",
    "    # Train\n",
    "    seed=777\n",
    "    lr=1e-5\n",
    "    weight_decay=0.2\n",
    "    epochs=4\n",
    "    gradient_accumulation_steps=1\n",
    "    batch_size=4\n",
    "    save_total_limit=0\n",
    "\n",
    "if CFG.type==\"target\":\n",
    "    CFG.num_classes=3\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(CFG.gpu_id)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from crowdkit.aggregation import GLAD, DawidSkene, MajorityVote\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          Trainer, TrainingArguments)\n",
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions for scoring\n",
    "# ====================================================\n",
    "def softmax(x):\n",
    "    f = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    return f\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if labels.ndim == 2:\n",
    "        labels = np.argmax(labels, axis=1)\n",
    "    acc = accuracy_score(labels, np.argmax(predictions, axis=1))\n",
    "    f1 = f1_score(labels, np.argmax(predictions, axis=1), average=\"macro\")\n",
    "    auc = roc_auc_score(\n",
    "        np.identity(4)[labels],\n",
    "        softmax(predictions),\n",
    "        multi_class=\"ovr\",\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (7119, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ロシアは異常な国‼️</td>\n",
       "      <td>[0.01905700520092961, 0.000806961516404502, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>いや、、怪しいな、、。第三者が「こっちだよ、こっちに友達いたよ」て誘導したとしか思えない、、。</td>\n",
       "      <td>[0.9618334745855203, 0.0004892807042054704, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>風評被害なかったの？状況知らないけど、自分なら管理出来ないなら飼うな。って、当たり前にレスな...</td>\n",
       "      <td>[0.9618334745855203, 0.0004892807042054704, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>もう何言ってるのか分からん状態になってきたな。。側近も止められないのかな・・・「さすがにそん...</td>\n",
       "      <td>[0.9618334745855203, 0.0004892807042054704, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>「耐えなけば...」と言う町長。訳が分からない！住民にこれだけ損害を与えておいて、どのような...</td>\n",
       "      <td>[0.9618334745855203, 0.0004892807042054704, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0                                         ロシアは異常な国‼️   \n",
       "1    いや、、怪しいな、、。第三者が「こっちだよ、こっちに友達いたよ」て誘導したとしか思えない、、。   \n",
       "2  風評被害なかったの？状況知らないけど、自分なら管理出来ないなら飼うな。って、当たり前にレスな...   \n",
       "3  もう何言ってるのか分からん状態になってきたな。。側近も止められないのかな・・・「さすがにそん...   \n",
       "4  「耐えなけば...」と言う町長。訳が分からない！住民にこれだけ損害を与えておいて、どのような...   \n",
       "\n",
       "                                               label  \n",
       "0  [0.01905700520092961, 0.000806961516404502, 0....  \n",
       "1  [0.9618334745855203, 0.0004892807042054704, 0....  \n",
       "2  [0.9618334745855203, 0.0004892807042054704, 0....  \n",
       "3  [0.9618334745855203, 0.0004892807042054704, 0....  \n",
       "4  [0.9618334745855203, 0.0004892807042054704, 0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape: (1448, 24)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>中国人の健康保険タダ乗り出来なくなるからこれはいいワクチン打ちに行ったら、日本語怪しいアジア...</td>\n",
       "      <td>[0.3605458931966271, 2.401694192479152e-15, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>犯罪国家を国連は殲滅せよ！！</td>\n",
       "      <td>[3.700261900179377e-05, 0.9987935254549858, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>町の責任で、若者の人生狂わせたとか同情の声があるが、自分は遅かれ早かれ犯罪は犯したと思う。人...</td>\n",
       "      <td>[0.09318441187853412, 0.0004033875637437423, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>また悲惨な事故が起こる前に高齢者の免許強制返納させた方がいい運転する資格ない</td>\n",
       "      <td>[0.3312375860956289, 8.635079984672648e-15, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>北を早く滅ぼそう</td>\n",
       "      <td>[3.700261900179377e-05, 0.9987935254549858, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  中国人の健康保険タダ乗り出来なくなるからこれはいいワクチン打ちに行ったら、日本語怪しいアジア...   \n",
       "1                                     犯罪国家を国連は殲滅せよ！！   \n",
       "2  町の責任で、若者の人生狂わせたとか同情の声があるが、自分は遅かれ早かれ犯罪は犯したと思う。人...   \n",
       "3             また悲惨な事故が起こる前に高齢者の免許強制返納させた方がいい運転する資格ない   \n",
       "4                                           北を早く滅ぼそう   \n",
       "\n",
       "                                               label  \n",
       "0  [0.3605458931966271, 2.401694192479152e-15, 0....  \n",
       "1  [3.700261900179377e-05, 0.9987935254549858, 0....  \n",
       "2  [0.09318441187853412, 0.0004033875637437423, 0...  \n",
       "3  [0.3312375860956289, 8.635079984672648e-15, 0....  \n",
       "4  [3.700261900179377e-05, 0.9987935254549858, 0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "\n",
    "# sample code from https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/Tweet-Lookup/get_tweets_with_bearer_token.py\n",
    "\n",
    "\n",
    "def create_url(ids: list):\n",
    "    tweet_fields = \"tweet.fields=created_at\"\n",
    "    ids = f\"ids={','.join(ids)}\"\n",
    "    url = \"https://api.twitter.com/2/tweets?{}&{}\".format(ids, tweet_fields)\n",
    "    return url\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {CFG.twitter_bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2TweetLookupPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url):\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_text_data(examples):\n",
    "    url = create_url(examples[\"id\"])\n",
    "    json_response = connect_to_endpoint(url)\n",
    "    # print(json_response[\"data\"])\n",
    "    text_dict = {data[\"id\"]: data[\"text\"] for data in json_response[\"data\"]}\n",
    "    time_dict = {data[\"id\"]: data[\"created_at\"] for data in json_response[\"data\"]}\n",
    "    return {\"text\": [text_dict.get(id) for id in examples[\"id\"]], \"created_at\": [time_dict.get(id) for id in examples[\"id\"]]}\n",
    "\n",
    "def get_dataset(train_path = \"train.pkl\", test_path = \"test.pkl\"):\n",
    "    if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "        return pd.read_pickle(train_path), pd.read_pickle(test_path)\n",
    "    else:\n",
    "        dataset = load_dataset(\"kubota/defamation-japanese-twitter\")\n",
    "        dataset = dataset.map(get_text_data, batched=True, batch_size=100)\n",
    "        \n",
    "        # 欠損(元ツイートが削除されているもの)を削除\n",
    "        df = dataset[\"train\"].to_pandas().dropna()\n",
    "        # 全員がことなるもの，2名以上がCを選択したものを排除\n",
    "        df = df[df[\"label\"].apply(lambda l: np.median(l) != 0.0 if len(set(l)) != len(l) else False)]\n",
    "        # ラベル統合のために変形\n",
    "        d_target = dict(\n",
    "            worker = pd.concat([df[\"user_id_list\"].apply(lambda x:x[i]) for i in range(3)]),\n",
    "            task = df[\"id\"].to_list()*3,\n",
    "            label = pd.concat([df[\"target\"].apply(lambda x:x[i]) for i in range(3)]),\n",
    "        )\n",
    "        d_target = pd.DataFrame.from_dict(d_target)\n",
    "        d_target[\"label\"].replace({3:0}, inplace=True)\n",
    "        d_label = dict(\n",
    "            worker = pd.concat([df[\"user_id_list\"].apply(lambda x:x[i]) for i in range(3)]),\n",
    "            task = df[\"id\"].to_list()*3,\n",
    "            label = pd.concat([df[\"label\"].apply(lambda x:x[i]) for i in range(3)]),\n",
    "        )\n",
    "        d_label = pd.DataFrame.from_dict(d_label)\n",
    "        d_label[\"label\"].replace({4:0}, inplace=True)\n",
    "        \n",
    "        # 誹謗中傷の対象，種類ごとにそれぞれラベル統合\n",
    "        df[\"mv_hard_target\"] = list(MajorityVote().fit_predict(d_target))\n",
    "        df[\"ds_hard_target\"] = list(DawidSkene(n_iter=200).fit_predict(d_target))\n",
    "        df[\"gl_hard_target\"] = list(GLAD(n_iter=200).fit_predict(d_target))\n",
    "        df[\"mv_soft_target\"] = MajorityVote().fit_predict_proba(d_target).to_numpy().tolist()\n",
    "        df[\"ds_soft_target\"] = DawidSkene(n_iter=200).fit_predict_proba(d_target).to_numpy().tolist()\n",
    "        df[\"gl_soft_target\"] = GLAD(n_iter=200).fit_predict_proba(d_target).to_numpy().tolist()\n",
    "        df[\"mv_hard_label\"] = list(MajorityVote().fit_predict(d_label))\n",
    "        df[\"ds_hard_label\"] = list(DawidSkene(n_iter=200).fit_predict(d_label))\n",
    "        df[\"gl_hard_label\"] = list(GLAD(n_iter=200).fit_predict(d_label))\n",
    "        df[\"mv_soft_label\"] = MajorityVote().fit_predict_proba(d_label).to_numpy().tolist()\n",
    "        df[\"ds_soft_label\"] = DawidSkene(n_iter=200).fit_predict_proba(d_label).to_numpy().tolist()\n",
    "        df[\"gl_soft_label\"] = GLAD(n_iter=200).fit_predict_proba(d_label).to_numpy().tolist()\n",
    "        display(df.groupby(\"mv_hard_target\").count()[\"id\"])\n",
    "        display(df.groupby(\"mv_hard_label\").count()[\"id\"])\n",
    "        \n",
    "        # 学習データ，テストデータにそれぞれ分割\n",
    "        train_df = df.query(\"created_at < '2022-05-21 00:00:00+00:00'\").reset_index(drop=True)\n",
    "        test_df = df.query(\"created_at > '2022-05-21 00:00:00+00:00'\").reset_index(drop=True)\n",
    "        train_df.to_pickle(train_path)\n",
    "        test_df.to_pickle(test_path)\n",
    "        return train_df, test_df\n",
    "\n",
    "train_df, test_df = get_dataset()\n",
    "train_df = train_df.drop(\"label\", axis=1).rename(columns={f\"{CFG.agg_type}_{CFG.label_type}_{CFG.type}\":\"label\"})\n",
    "test_df = test_df.drop(\"label\", axis=1).rename(columns={f\"{CFG.agg_type}_{CFG.label_type}_{CFG.type}\":\"label\"})\n",
    "\n",
    "print(f\"train.shape: {train_df.shape}\")\n",
    "display(train_df[[\"text\", \"label\"]].head())\n",
    "print(f\"test.shape: {test_df.shape}\")\n",
    "display(test_df[[\"text\", \"label\"]].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7119/7119 [00:01<00:00, 4247.70it/s]\n",
      "100%|██████████| 1448/1448 [00:01<00:00, 727.70it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text max(lengths): 472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Tokenizer\n",
    "# ====================================================\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "text_lengths = []\n",
    "tk0 = tqdm(train_df[\"text\"].fillna(\"\").values, total=len(train_df))\n",
    "tk1 = tqdm(test_df[\"text\"].fillna(\"\").values, total=len(test_df))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    text_lengths.append(length)\n",
    "for text in tk1:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    text_lengths.append(length)\n",
    "\n",
    "CFG.max_len = max(text_lengths) + 2 # CLS + SEP\n",
    "print(f'Text max(lengths): {max(text_lengths)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b925ea92f140438cbfc6b7bde8a1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd5c3544e9a94bb7a47731766e0e5e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_pandas(train_df),\n",
    "        \"test\": Dataset.from_pandas(test_df),\n",
    "    }\n",
    ")\n",
    "\n",
    "def clean_text(text):\n",
    "    return (\n",
    "        text.replace(\" \", \"\")\n",
    "        .replace(\"@user\", \"\")\n",
    "        .replace(\"　\", \"\")\n",
    "        .replace(\"__BR__\", \"\\n\")\n",
    "        .replace(\"\\xa0\", \"\")\n",
    "        .replace(\"\\r\", \"\")\n",
    "        .lstrip(\"\\n\")\n",
    "    )\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        list(map(clean_text, batch[\"text\"])),\n",
    "        add_special_tokens=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_len,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at studio-ousia/luke-japanese-large were not used when initializing LukeForSequenceClassification: ['entity_predictions.transform.dense.bias', 'lm_head.dense.weight', 'entity_predictions.transform.LayerNorm.bias', 'lm_head.layer_norm.weight', 'entity_predictions.bias', 'entity_predictions.transform.LayerNorm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'entity_predictions.transform.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LukeForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LukeForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LukeForSequenceClassification were not initialized from the model checkpoint at studio-ousia/luke-japanese-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1780' max='1780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1780/1780 17:01, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.421100</td>\n",
       "      <td>0.379707</td>\n",
       "      <td>0.675414</td>\n",
       "      <td>0.633065</td>\n",
       "      <td>0.864727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.336200</td>\n",
       "      <td>0.383698</td>\n",
       "      <td>0.671961</td>\n",
       "      <td>0.622264</td>\n",
       "      <td>0.865230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.284200</td>\n",
       "      <td>0.396729</td>\n",
       "      <td>0.676105</td>\n",
       "      <td>0.648729</td>\n",
       "      <td>0.868334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.255400</td>\n",
       "      <td>0.405629</td>\n",
       "      <td>0.671271</td>\n",
       "      <td>0.644920</td>\n",
       "      <td>0.866171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1780, training_loss=0.3240535478913382, metrics={'train_runtime': 1022.6499, 'train_samples_per_second': 27.845, 'train_steps_per_second': 1.741, 'total_flos': 3.0709835913443904e+16, 'train_loss': 0.3240535478913382, 'epoch': 4.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Training\n",
    "# ====================================================\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CFG.model, num_labels=CFG.num_classes\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CFG.output_dir,\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    learning_rate=CFG.lr,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    lr_scheduler_type=CFG.scheduler_type,\n",
    "    disable_tqdm=False,\n",
    "    save_total_limit=CFG.save_total_limit,\n",
    "    logging_steps=len(dataset_encoded[\"train\"]) // CFG.batch_size,\n",
    "    push_to_hub=False,\n",
    "    log_level=\"error\",\n",
    "    fp16=True,\n",
    "    seed=CFG.seed,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ds_hard_label</th>\n",
       "      <th>pred_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>中国人の健康保険タダ乗り出来なくなるからこれはいいワクチン打ちに行ったら、日本語怪しいアジア...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>犯罪国家を国連は殲滅せよ！！</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>町の責任で、若者の人生狂わせたとか同情の声があるが、自分は遅かれ早かれ犯罪は犯したと思う。人...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>また悲惨な事故が起こる前に高齢者の免許強制返納させた方がいい運転する資格ない</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>北を早く滅ぼそう</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  ds_hard_label  \\\n",
       "0  中国人の健康保険タダ乗り出来なくなるからこれはいいワクチン打ちに行ったら、日本語怪しいアジア...              3   \n",
       "1                                     犯罪国家を国連は殲滅せよ！！              1   \n",
       "2  町の責任で、若者の人生狂わせたとか同情の声があるが、自分は遅かれ早かれ犯罪は犯したと思う。人...              3   \n",
       "3             また悲惨な事故が起こる前に高齢者の免許強制返納させた方がいい運転する資格ない              3   \n",
       "4                                           北を早く滅ぼそう              1   \n",
       "\n",
       "   pred_label  \n",
       "0           0  \n",
       "1           1  \n",
       "2           0  \n",
       "3           1  \n",
       "4           1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = trainer.predict(dataset_encoded[\"test\"])\n",
    "test_df[\"pred_logits\"] = softmax(output.predictions).tolist()\n",
    "test_df[f\"pred_{CFG.type}\"] = softmax(output.predictions).argmax(1)\n",
    "test_df[[\"text\", f\"{CFG.agg_type}_hard_{CFG.type}\", f\"pred_{CFG.type}\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7222d7283ab5a383\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7222d7283ab5a383\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir output/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
