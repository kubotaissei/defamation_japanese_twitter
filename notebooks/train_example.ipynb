{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Sample Scripts Using Huggingface Trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kubotaissei/defamation_japanese_twitter/blob/master/notebooks/train_example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.26 datasets==2.8.0 sentencepiece crowd-kit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    debug=False\n",
    "    # base\n",
    "    gpu_id=[\"0\"]\n",
    "    output_dir=\"output\"\n",
    "    twitter_bearer_token=\"\"  # Fill in your Twitter API bearer token\n",
    "    # Dataset\n",
    "    agg_type=\"mv\" #[mv, ds, gl]\n",
    "    type=\"label\" #[label, target]\n",
    "    label_type=\"hard\" #[hard, soft]\n",
    "    max_len=497\n",
    "    # Model\n",
    "    model=\"studio-ousia/luke-japanese-large\"\n",
    "    num_classes=4\n",
    "    # Scheduler\n",
    "    scheduler_type=\"cosine\"\n",
    "    # Train\n",
    "    seed=777\n",
    "    lr=1e-5\n",
    "    weight_decay=0.2\n",
    "    epochs=4\n",
    "    gradient_accumulation_steps=1\n",
    "    batch_size=4\n",
    "    save_total_limit=0\n",
    "    \n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \",\".join(CFG.gpu_id)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from crowdkit.aggregation import GLAD, DawidSkene, MajorityVote\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import (AutoModelForSequenceClassification, AutoTokenizer,\n",
    "                          Trainer, TrainingArguments)\n",
    "%env TOKENIZERS_PARALLELISM=true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions for scoring\n",
    "# ====================================================\n",
    "def softmax(x):\n",
    "    f = np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "    return f\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    acc = accuracy_score(labels, np.argmax(predictions, axis=1))\n",
    "    f1 = f1_score(labels, np.argmax(predictions, axis=1), average=\"macro\")\n",
    "    auc = roc_auc_score(\n",
    "        np.identity(4)[labels],\n",
    "        softmax(predictions),\n",
    "        multi_class=\"ovr\",\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (3122, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ジェンダーとかそういうのは関係なく、男の子はこんなもん、女の子だからこういうもん、そういう言...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@sharenewsjapan1 河村は阿呆か？\\n今韓国と仲良くお手てつないでなどできるは...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>低能共がドヤ顔で写真撮る権利なんて要らねえよ。\\n糞どもはやく死なねえかな。\\n鉄道は静かに...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ライル：（…消えてしまえ、消えてしまえ消えろ消えろ消えろ消えろっ!!!!マフィアなんて全部、...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>自分が住んでいる都市は街全体がおっきな老人ホームみたいになっていて、老人の住みやすさが優先さ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  ジェンダーとかそういうのは関係なく、男の子はこんなもん、女の子だからこういうもん、そういう言...      0\n",
       "1  @sharenewsjapan1 河村は阿呆か？\\n今韓国と仲良くお手てつないでなどできるは...      2\n",
       "2  低能共がドヤ顔で写真撮る権利なんて要らねえよ。\\n糞どもはやく死なねえかな。\\n鉄道は静かに...      1\n",
       "3  ライル：（…消えてしまえ、消えてしまえ消えろ消えろ消えろ消えろっ!!!!マフィアなんて全部、...      1\n",
       "4  自分が住んでいる都市は街全体がおっきな老人ホームみたいになっていて、老人の住みやすさが優先さ...      3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape: (879, 17)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@ReutersJapan 確かに侵略戦争で民間人を殺害したのは裁かれるべきですが、本来ロシ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>日本人が「劣化」したワケ…受験勉強で「学歴の高いバカ」が大量生産されている！ 中野信子氏と和...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>静岡県知事がリニアを通さない。これ自体が日本が中国に支配されていて日本を潰すのは施政者など力...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>日本に外国人を住まわせちゃいけない理由\\n\\n・最早区別は差別と同義\\n・別の国から来た人は...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>やられてもやり返すな。同類だぞみたいな日本人特有の文化なに？そんな考えのやつばっかだから日本...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  @ReutersJapan 確かに侵略戦争で民間人を殺害したのは裁かれるべきですが、本来ロシ...      0\n",
       "1  日本人が「劣化」したワケ…受験勉強で「学歴の高いバカ」が大量生産されている！ 中野信子氏と和...      0\n",
       "2  静岡県知事がリニアを通さない。これ自体が日本が中国に支配されていて日本を潰すのは施政者など力...      3\n",
       "3  日本に外国人を住まわせちゃいけない理由\\n\\n・最早区別は差別と同義\\n・別の国から来た人は...      0\n",
       "4  やられてもやり返すな。同類だぞみたいな日本人特有の文化なに？そんな考えのやつばっかだから日本...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "\n",
    "# sample code from https://github.com/twitterdev/Twitter-API-v2-sample-code/blob/main/Tweet-Lookup/get_tweets_with_bearer_token.py\n",
    "\n",
    "\n",
    "def create_url(ids: list):\n",
    "    tweet_fields = \"tweet.fields=created_at\"\n",
    "    ids = f\"ids={','.join(ids)}\"\n",
    "    url = \"https://api.twitter.com/2/tweets?{}&{}\".format(ids, tweet_fields)\n",
    "    return url\n",
    "\n",
    "\n",
    "def bearer_oauth(r):\n",
    "    \"\"\"\n",
    "    Method required by bearer token authentication.\n",
    "    \"\"\"\n",
    "\n",
    "    r.headers[\"Authorization\"] = f\"Bearer {CFG.twitter_bearer_token}\"\n",
    "    r.headers[\"User-Agent\"] = \"v2TweetLookupPython\"\n",
    "    return r\n",
    "\n",
    "\n",
    "def connect_to_endpoint(url):\n",
    "    response = requests.request(\"GET\", url, auth=bearer_oauth)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(\n",
    "            \"Request returned an error: {} {}\".format(\n",
    "                response.status_code, response.text\n",
    "            )\n",
    "        )\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def get_text_data(examples):\n",
    "    url = create_url(examples[\"id\"])\n",
    "    json_response = connect_to_endpoint(url)\n",
    "    # print(json_response[\"data\"])\n",
    "    text_dict = {data[\"id\"]: data[\"text\"] for data in json_response[\"data\"]}\n",
    "    time_dict = {data[\"id\"]: data[\"created_at\"] for data in json_response[\"data\"]}\n",
    "    return {\"text\": [text_dict.get(id) for id in examples[\"id\"]], \"created_at\": [time_dict.get(id) for id in examples[\"id\"]]}\n",
    "\n",
    "def get_dataset(train_path = \"train.pkl\", test_path = \"test.pkl\"):\n",
    "    if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "        return pd.read_pickle(train_path), pd.read_pickle(test_path)\n",
    "    else:\n",
    "        dataset = load_dataset(\"kubota/defamation-japanese-twitter\")\n",
    "        dataset = dataset.map(get_text_data, batched=True, batch_size=100)\n",
    "        \n",
    "        # 欠損(元ツイートが削除されているもの)を削除\n",
    "        df = dataset[\"train\"].to_pandas().dropna()\n",
    "        # 全員がことなるもの，2名以上がCを選択したものを排除\n",
    "        df = df[df[\"label\"].apply(lambda l: np.median(l) != 0.0 if len(set(l)) != len(l) else False)]\n",
    "        # ラベル統合のために変形\n",
    "        d_target = dict(\n",
    "            worker = pd.concat([df[\"user_id_list\"].apply(lambda x:x[i]) for i in range(3)]),\n",
    "            task = df[\"id\"].to_list()*3,\n",
    "            label = pd.concat([df[\"target\"].apply(lambda x:x[i]) for i in range(3)]),\n",
    "        )\n",
    "        d_target = pd.DataFrame.from_dict(d_target)\n",
    "        d_target[\"label\"].replace({3:0}, inplace=True)\n",
    "        d_label = dict(\n",
    "            worker = pd.concat([df[\"user_id_list\"].apply(lambda x:x[i]) for i in range(3)]),\n",
    "            task = df[\"id\"].to_list()*3,\n",
    "            label = pd.concat([df[\"label\"].apply(lambda x:x[i]) for i in range(3)]),\n",
    "        )\n",
    "        d_label = pd.DataFrame.from_dict(d_label)\n",
    "        d_label[\"label\"].replace({4:0}, inplace=True)\n",
    "        \n",
    "        # 誹謗中傷の対象，種類ごとにそれぞれラベル統合\n",
    "        df[\"mv_hard_target\"] = list(MajorityVote().fit_predict(d_target))\n",
    "        df[\"ds_hard_target\"] = list(DawidSkene(n_iter=200).fit_predict(d_target))\n",
    "        df[\"gl_hard_target\"] = list(GLAD(n_iter=200).fit_predict(d_target))\n",
    "        df[\"mv_soft_target\"] = MajorityVote().fit_predict_proba(d_target).to_numpy().tolist()\n",
    "        df[\"ds_soft_target\"] = DawidSkene(n_iter=200).fit_predict_proba(d_target).to_numpy().tolist()\n",
    "        df[\"gl_soft_target\"] = GLAD(n_iter=200).fit_predict_proba(d_target).to_numpy().tolist()\n",
    "        df[\"mv_hard_label\"] = list(MajorityVote().fit_predict(d_label))\n",
    "        df[\"ds_hard_label\"] = list(DawidSkene(n_iter=200).fit_predict(d_label))\n",
    "        df[\"gl_hard_label\"] = list(GLAD(n_iter=200).fit_predict(d_label))\n",
    "        df[\"mv_soft_label\"] = MajorityVote().fit_predict_proba(d_label).to_numpy().tolist()\n",
    "        df[\"ds_soft_label\"] = DawidSkene(n_iter=200).fit_predict_proba(d_label).to_numpy().tolist()\n",
    "        df[\"gl_soft_label\"] = GLAD(n_iter=200).fit_predict_proba(d_label).to_numpy().tolist()\n",
    "        display(df.groupby(\"mv_hard_target\").count()[\"id\"])\n",
    "        display(df.groupby(\"mv_hard_label\").count()[\"id\"])\n",
    "        \n",
    "        # 学習データ，テストデータにそれぞれ分割\n",
    "        train_df = df.query(\"created_at < '2022-05-21 00:00:00+00:00'\").reset_index(drop=True)\n",
    "        test_df = df.query(\"created_at > '2022-05-21 00:00:00+00:00'\").reset_index(drop=True)\n",
    "        train_df.to_pickle(train_path)\n",
    "        test_df.to_pickle(test_path)\n",
    "        return train_df, test_df\n",
    "\n",
    "train_df, test_df = get_dataset()\n",
    "train_df = train_df.drop(\"label\", axis=1).rename(columns={f\"{CFG.agg_type}_{CFG.label_type}_{CFG.type}\":\"label\"})\n",
    "test_df = test_df.drop(\"label\", axis=1).rename(columns={f\"{CFG.agg_type}_{CFG.label_type}_{CFG.type}\":\"label\"})\n",
    "\n",
    "print(f\"train.shape: {train_df.shape}\")\n",
    "display(train_df[[\"text\", \"label\"]].head())\n",
    "print(f\"test.shape: {test_df.shape}\")\n",
    "display(test_df[[\"text\", \"label\"]].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3122/3122 [00:00<00:00, 4245.52it/s]\n",
      "100%|██████████| 879/879 [00:00<00:00, 959.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text max(lengths): 495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Tokenizer\n",
    "# ====================================================\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "text_lengths = []\n",
    "tk0 = tqdm(train_df[\"text\"].fillna(\"\").values, total=len(train_df))\n",
    "tk1 = tqdm(test_df[\"text\"].fillna(\"\").values, total=len(test_df))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    text_lengths.append(length)\n",
    "for text in tk1:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    text_lengths.append(length)\n",
    "\n",
    "CFG.max_len = max(text_lengths) + 2 # CLS + SEP\n",
    "print(f'Text max(lengths): {max(text_lengths)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38f94dd42604ca29e55b49d1d89fbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511e639ef2c847ac89b0585b216dbbf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "dataset = DatasetDict(\n",
    "    {\n",
    "        \"train\": Dataset.from_pandas(train_df),\n",
    "        \"test\": Dataset.from_pandas(test_df),\n",
    "    }\n",
    ")\n",
    "\n",
    "def clean_text(text):\n",
    "    return (\n",
    "        text.replace(\" \", \"\")\n",
    "        .replace(\"@user\", \"\")\n",
    "        .replace(\"　\", \"\")\n",
    "        .replace(\"__BR__\", \"\\n\")\n",
    "        .replace(\"\\xa0\", \"\")\n",
    "        .replace(\"\\r\", \"\")\n",
    "        .lstrip(\"\\n\")\n",
    "    )\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(\n",
    "        list(map(clean_text, batch[\"text\"])),\n",
    "        add_special_tokens=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=CFG.max_len,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at studio-ousia/luke-japanese-large were not used when initializing LukeForSequenceClassification: ['lm_head.layer_norm.bias', 'entity_predictions.transform.LayerNorm.weight', 'entity_predictions.transform.LayerNorm.bias', 'lm_head.dense.weight', 'entity_predictions.transform.dense.bias', 'entity_predictions.bias', 'entity_predictions.transform.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing LukeForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LukeForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LukeForSequenceClassification were not initialized from the model checkpoint at studio-ousia/luke-japanese-large and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='784' max='784' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [784/784 05:18, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.017800</td>\n",
       "      <td>0.849494</td>\n",
       "      <td>0.641638</td>\n",
       "      <td>0.596703</td>\n",
       "      <td>0.836809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697000</td>\n",
       "      <td>0.974318</td>\n",
       "      <td>0.648464</td>\n",
       "      <td>0.607049</td>\n",
       "      <td>0.827209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.498500</td>\n",
       "      <td>1.033791</td>\n",
       "      <td>0.642776</td>\n",
       "      <td>0.607933</td>\n",
       "      <td>0.833853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.372900</td>\n",
       "      <td>1.084045</td>\n",
       "      <td>0.650739</td>\n",
       "      <td>0.615489</td>\n",
       "      <td>0.831961</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=784, training_loss=0.6448173109365969, metrics={'train_runtime': 319.8105, 'train_samples_per_second': 39.048, 'train_steps_per_second': 2.451, 'total_flos': 9120488340374208.0, 'train_loss': 0.6448173109365969, 'epoch': 4.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Training\n",
    "# ====================================================\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CFG.model, num_labels=CFG.num_classes\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=CFG.output_dir,\n",
    "    num_train_epochs=CFG.epochs,\n",
    "    learning_rate=CFG.lr,\n",
    "    per_device_train_batch_size=CFG.batch_size,\n",
    "    per_device_eval_batch_size=CFG.batch_size,\n",
    "    weight_decay=CFG.weight_decay,\n",
    "    gradient_accumulation_steps=CFG.gradient_accumulation_steps,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    lr_scheduler_type=CFG.scheduler_type,\n",
    "    disable_tqdm=False,\n",
    "    save_total_limit=CFG.save_total_limit,\n",
    "    logging_steps=len(dataset_encoded[\"train\"]) // CFG.batch_size,\n",
    "    push_to_hub=False,\n",
    "    log_level=\"error\",\n",
    "    fp16=True,\n",
    "    seed=CFG.seed,\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=dataset_encoded[\"train\"],\n",
    "    eval_dataset=dataset_encoded[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
